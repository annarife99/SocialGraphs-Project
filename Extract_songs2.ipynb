{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4314a20",
   "metadata": {},
   "source": [
    "# <font color='darkblue'> THE NETWORK OF BEATLES' SONGS\n",
    "\n",
    "## <font color='darkblue'>Analysis of the network of Beatles'Songs: Getting into their mind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c3f60f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We import all the necessary packages we used for this project\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import operator\n",
    "import powerlaw\n",
    "from fa2 import ForceAtlas2\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "import urllib.request as urllib2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7365af0a",
   "metadata": {},
   "source": [
    "###  <font color='darkblue'>Part 1a: Extraction of information from Beatles Official Page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2af31560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have create a function to obtain the 'HTLM' of Beatles Official Page. \n",
    "def extract_html(url):\n",
    "    page = urlopen(url)\n",
    "    html_bytes = page.read()\n",
    "    html = html_bytes.decode(\"utf-8\")\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca7c5c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The official Beatles page contains information of a number of 301 songs.\n"
     ]
    }
   ],
   "source": [
    "#We obtain all the list of songs and we stored it in the Variable Songs_Titles\n",
    "pattern1= 'hreflang=\"en\">(.*?)</a>'\n",
    "songs_titles=[]\n",
    "for i in range(0,9): \n",
    "    url = 'https://www.thebeatles.com/songs?page='+str(i)\n",
    "    html=extract_html(url)\n",
    "    a=re.findall(pattern1,html)\n",
    "    for el in a:\n",
    "        el=el.replace('&#039;', '\\'')\n",
    "        songs_titles.append(el)\n",
    "\n",
    "print('The official Beatles page contains information of a number of' , len(songs_titles), 'songs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8986f63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_list=[' a ',' in ',' the ',' of ',' to ',' is ',' at ',' for ',' that ',' by ',' as ',' from ',' into ',\n",
    "             ' on ',' with ',' off ',' this ',' up ',' like ']\n",
    "remove_list_start=['a-','in-','the-','of-','to-','is-','at-','for-','that-','by-','as-','from-','like-','this-',\n",
    "                  'with-']\n",
    "remove_list_end=['-by','-to','-on','-is','-that','-before']\n",
    "remove_list2=['\\'','.','!','(',')',',','/',':']\n",
    "\n",
    "pattern6='<div class=\"col-md-6 middle-content border-left border-right\"><p>'\n",
    "pattern7='<figure class=\"wp-block-table table-expander table table-imported\">'\n",
    "for title in songs_titles:\n",
    "    try:\n",
    "        title=title.lower()\n",
    "        for el in remove_list2: # delete special characters\n",
    "            title=title.replace(el,'')\n",
    "        for el in remove_list: # replace single words with a space\n",
    "            title=title.replace(el,' ')\n",
    "        title=title.replace(' ','-')\n",
    "        title=title.replace('--','-')\n",
    "        # delete words from the start\n",
    "        for i in range(0,6):\n",
    "            if title[:i] in remove_list_start:\n",
    "                title=title[i:]\n",
    "        # delete words end\n",
    "        for i in range(0,8):\n",
    "            if title[-i:] in remove_list_end:\n",
    "                title=title[:-i]\n",
    "        if title[0]=='-':\n",
    "            title=title[1:]\n",
    "        url='https://www.thebeatles.com/'\n",
    "        query=extract_html(url+title)\n",
    "        # write the extracted text in a .txt file\n",
    "        file=open('raw_songs/'+title+'.txt','w+')\n",
    "        file.write(query)\n",
    "        file.close()\n",
    "    except:\n",
    "        print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a79f050",
   "metadata": {},
   "source": [
    "Hi ha a vegades que et pila la query del url tot i que la pàgina que correspon a la cançó té un afegit al final de -0 o -1, com el cas de la cançó que peta que és words-love-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e0faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern6='<div class=\"col-md-6 middle-content border-left border-right\"><p>'\n",
    "pattern7='<figure class=\"wp-block-table table-expander table table-imported\">'\n",
    "remove_list=['<br />','\\n','</p>','<p>']\n",
    "\n",
    "def extract_lyrics(song_path):\n",
    "    song=open(song_path).read()\n",
    "    idx_init = re.search(pattern6,song).end()\n",
    "    idx_final = re.search(pattern7,song).start()\n",
    "    lyrics=song[idx_init:idx_final]\n",
    "    for el in remove_list: # delete special characters\n",
    "        lyrics=lyrics.replace(el,' ')\n",
    "    return lyrics\n",
    "\n",
    "extract_lyrics('raw_songs/ob-la-di-ob-la-da.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9e0ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_lyrics('raw_songs/yesterday.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7fec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_files = os.listdir('raw_songs')\n",
    "songs_no_lyrics=[]\n",
    "for el in txt_files:\n",
    "    el=el[:-4]\n",
    "    song_path='raw_songs/'+el+'.txt'\n",
    "    try:\n",
    "        lyrics=extract_lyrics(song_path)\n",
    "        file=open('lyrics_songs/'+el+'.txt','w+')\n",
    "        file.write(lyrics)\n",
    "        file.close()\n",
    "    except:\n",
    "        songs_no_lyrics.append(el)\n",
    "        print(el, end='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dd8519",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_files = os.listdir('lyrics_songs')\n",
    "for el in songs_no_lyrics:\n",
    "    el=el[:-4]\n",
    "    url='https://www.thebeatles.com/'\n",
    "    # check if the song has lyrics or not\n",
    "    for i in range(0,4):\n",
    "        try:\n",
    "            query=extract_html(url+el+'-'+str(i))\n",
    "            idx_init = re.search(pattern6,query).end()\n",
    "            idx_final = re.search(pattern7,query).start()\n",
    "            # rewrite the extracted text in a .txt file\n",
    "            file=open('raw_songs/'+el+'.txt','w+')\n",
    "            file.write(query)\n",
    "            file.close()\n",
    "            print(el, i)\n",
    "            break\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bba4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_files = os.listdir('raw_songs')\n",
    "songs_no_lyrics=[]\n",
    "for el in txt_files:\n",
    "    song_path='raw_songs/'+el\n",
    "    try:\n",
    "        lyrics=extract_lyrics(song_path)\n",
    "        file=open('lyrics_songs/'+el+'.txt','w+')\n",
    "        file.write(lyrics)\n",
    "        file.close()\n",
    "    except:\n",
    "        songs_no_lyrics.append(el)\n",
    "len(songs_no_lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8978cb",
   "metadata": {},
   "source": [
    "### Feature: Clean lyrics files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddada72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "stop_words = [el.replace('\\'','') for el in stop_words]\n",
    "stop_words.append('im')\n",
    "\n",
    "\n",
    "def clean_lyrics(file_path):\n",
    "    data=open(file_path).read()\n",
    "    # import WordPunctTokenizer() method from nltk\n",
    "    # Create a reference variable for Class WordPunctTokenizer\n",
    "    tk = WordPunctTokenizer()\n",
    "    # define punctuation\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    # remove punctuation from the string\n",
    "    no_punct = \"\"\n",
    "    for char in data:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char\n",
    "    # Remove all the special characters as \\n and single = left\n",
    "    char=['\\n','=']\n",
    "    for el in char:\n",
    "        raw=re.sub(el,'',no_punct)\n",
    "    # remove stop words\n",
    "    token_txt = tk.tokenize(raw.lower()) # set to lower case\n",
    "    token_txt = tk.tokenize(raw.lower()) # set to lower case\n",
    "    token_final = [x for x in token_txt if x not in stop_words and len(x)>2]\n",
    "    return token_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b5169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_files=[el[:-4] for el in os.listdir('lyrics_songs')]\n",
    "lyrics_files.remove('.DS_S') # remove this element that is introduced when using os.listdir\n",
    "len(lyrics_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e2bd0f",
   "metadata": {},
   "source": [
    "## Create network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dc20b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_top5={}\n",
    "for file in lyrics_files:\n",
    "    path='lyrics_songs/'+file+'.txt'\n",
    "    clean=clean_lyrics(path)\n",
    "    \n",
    "    top5=[]\n",
    "    for el in FreqDist(clean).most_common(5):\n",
    "        top5.append(el[0])\n",
    "    d_top5[file]=top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef5a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperlinks={} #dictionary to store links \n",
    "for file1 in lyrics_files:\n",
    "    eq_files=[]\n",
    "    for file2 in lyrics_files:\n",
    "        if file1!=file2:\n",
    "            a=d_top5[file1]\n",
    "            b=d_top5[file2]\n",
    "            # compute if there are equal words in both lists\n",
    "            eq=0\n",
    "            for el in a:\n",
    "                if el in b:\n",
    "                    eq+=1\n",
    "            if eq!=0:\n",
    "                eq_files.append(file2)\n",
    "    hyperlinks[file1]=eq_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff32afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "#We build the Directed Network\n",
    "Beatles_nw= nx.DiGraph()\n",
    "for file in lyrics_files:\n",
    "    #We add the nodes and attributes to the the network\n",
    "    Beatles_nw.add_node(file)\n",
    "\n",
    "\n",
    "#We add the hyperlinks to the the network\n",
    "for el in lyrics_files:\n",
    "    links= hyperlinks[el]\n",
    "    for a in links:\n",
    "        u= el\n",
    "        v= a\n",
    "        Beatles_nw.add_edge(u,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b15c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(Beatles_nw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c94f4b",
   "metadata": {},
   "source": [
    "### Extract GCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06506339",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gs = list(Beatles_nw.subgraph(c).copy() for c in nx.weakly_connected_components(Beatles_nw))\n",
    "biggest = 0\n",
    "GCC_index = 0\n",
    "for index,graph in enumerate(Gs):\n",
    "    if len(graph.nodes) > biggest:\n",
    "        biggest = len(graph.nodes)\n",
    "        GCC_index = index\n",
    "GCC = Gs[GCC_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ce14f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    'node_size': 40,\n",
    "    'width': 0.2,\n",
    "}\n",
    "nx.draw(GCC,**options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492fe64a",
   "metadata": {},
   "source": [
    "#### Convert to undirected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a26cc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_Beatles_nw= GCC.to_undirected()\n",
    "\n",
    "options = {\n",
    "    'node_size': 40,\n",
    "    'width': 0.2,\n",
    "}\n",
    "nx.draw(un_Beatles_nw,**options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6785727",
   "metadata": {},
   "source": [
    "Still needs to be define a criteria to link two songs. Right now the criteria is that if between the most 5 common words one is present in both songs, these songs are linked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sorted_degree=dict(sorted(un_Beatles_nw.degree, key=lambda x: x[1], reverse=True))\n",
    "Sorted_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cfc6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_top5['dont-pass-me']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4a1aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_top5['love-me-do']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4665b22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_top5['i-feel-fine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7ea144",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_top5['yellow-submarine']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfe98a7",
   "metadata": {},
   "source": [
    "#### Plot degree distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa8189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#FROM UNDIRECTED TO DIRECTED GRAPH:\n",
    "un_Beatles_nw= GCC.to_undirected()\n",
    "all_degrees=[]\n",
    "\n",
    "#Obtenition of degree \n",
    "for deg in un_Beatles_nw.degree():\n",
    "    all_degrees.append(deg[1])\n",
    "    \n",
    "#Zoom in\n",
    "v = np.arange(int(min(all_degrees)),max(Sorted_degree.values()))\n",
    "count,bins=np.histogram(all_degrees,bins=v)\n",
    "plt.bar(bins[:-1], count)\n",
    "plt.title('Zoom in: Degree distribution of the undirected graph',fontsize=13)\n",
    "plt.xlabel('Degree',fontsize=12)\n",
    "plt.ylabel('Counts',fontsize=12)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c7f2f2",
   "metadata": {},
   "source": [
    "## Find communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610e1a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import community.community_louvain\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# compute the best partition\n",
    "partition = community.community_louvain.best_partition(un_Beatles_nw)\n",
    "\n",
    "# draw the graph\n",
    "pos = nx.spring_layout(un_Beatles_nw)\n",
    "# color the nodes according to their partition\n",
    "cmap = cm.get_cmap('viridis', max(partition.values()) + 1)\n",
    "nx.draw_networkx_nodes(un_Beatles_nw, pos, partition.keys(), node_size=20,\n",
    "                       cmap=cmap, node_color=list(partition.values()))\n",
    "nx.draw_networkx_edges(un_Beatles_nw, pos, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d849b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of communities found: ', len(np.unique(list(partition.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6373a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Modularity:', np.round(community.community_louvain.modularity(partition,un_Beatles_nw),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37693d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we compute the number of characters in every community in order to find the top 5\n",
    "communities_d=dict.fromkeys(list(range(0,len(np.unique(list(partition.values()))))))\n",
    "for i in range(0,len(np.unique(list(partition.values())))):\n",
    "    l=[]\n",
    "    for el in partition:\n",
    "        if partition[el]==i:\n",
    "            l.append(el)\n",
    "    communities_d[i]=l\n",
    "    \n",
    "size_communities=[len(x) for x in communities_d.values()]\n",
    "\n",
    "size_communities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cfcb7a",
   "metadata": {},
   "source": [
    "Extract the most common words for each community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26436cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_d[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a79094",
   "metadata": {},
   "source": [
    "Most common words in each community (counting the time each word appears)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c155b3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in communities_d:\n",
    "    community_words=[]\n",
    "    for el in communities_d[idx]:\n",
    "        path='lyrics_songs/'+el+'.txt'\n",
    "        clean=clean_lyrics(path)\n",
    "        for word in clean:\n",
    "            community_words.append(word)\n",
    "    print(FreqDist(community_words).most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0945b9ac",
   "metadata": {},
   "source": [
    "Most common words in each community (counting the number of song a word appear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in communities_d:\n",
    "    community_words=[]\n",
    "    for el in communities_d[idx]:\n",
    "        path='lyrics_songs/'+el+'.txt'\n",
    "        clean=clean_lyrics(path)\n",
    "        unique=list(np.unique(clean))\n",
    "        for word in unique:\n",
    "            community_words.append(word)\n",
    "    print('size:' ,size_communities[idx])\n",
    "    print(FreqDist(community_words).most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1890f48",
   "metadata": {},
   "source": [
    "### Feature: most common words of a given community (times a song contains a word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f61663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def community_words(community):\n",
    "    community_words=[]\n",
    "    for el in community:\n",
    "        path='lyrics_songs/'+el+'.txt'\n",
    "        clean=clean_lyrics(path)\n",
    "        unique=list(np.unique(clean))\n",
    "        for word in unique:\n",
    "            community_words.append(word)\n",
    "    return FreqDist(community_words).most_common(5)\n",
    "\n",
    "community_words(communities_d[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62193090",
   "metadata": {},
   "source": [
    "### Feature: introduce a famous song and find in which community it belongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3599cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def song_community(song):\n",
    "    i=0\n",
    "    for idx in communities_d:\n",
    "        if song in communities_d[i]:\n",
    "            return i\n",
    "        i+=1\n",
    "        \n",
    "song_community('let-it-be')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75165d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "community_words(communities_d[song_community('let-it-be')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a78758",
   "metadata": {},
   "source": [
    "## VADER sentimental analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744a297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43479c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceDic = {}\n",
    "for name in dialogues:\n",
    "    sentenceDic[name] = []\n",
    "    for line in dialogues[name]:\n",
    "        sentence = re.split(r'[.!?]+ *',line)\n",
    "        sentenceDic[name].extend(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f05b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('lyrics_songs/youll-be-mine.txt').read()\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec42b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[]\n",
    "index=[(m.start(0), m.end(0)) for m in re.finditer('  ',file)]\n",
    "for i in range(0,len(index)-1):\n",
    "    a=file[index[i][1]:index[i+1][0]]\n",
    "    sentences.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aff7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00531955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def song_Vader(song):\n",
    "    # extract sentence of each song\n",
    "    sentences=[]\n",
    "    index=[(m.start(0), m.end(0)) for m in re.finditer('  ',file)]\n",
    "    for i in range(0,len(index)-1):\n",
    "        a=file[index[i][1]:index[i+1][0]]\n",
    "        sentences.append(a)\n",
    "    # compute sentiment analysis for sentence\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    compound1=[]\n",
    "    compound2=[]\n",
    "    for sentence in sentences:\n",
    "        vs = analyzer.polarity_scores(sentence)\n",
    "        # take every sentence value (more robust)\n",
    "        compound1.append(vs['compound'])\n",
    "        # take just the sentences that are not neutral\n",
    "        if vs['compound']!=0:\n",
    "            compound2.append(vs['compound'])\n",
    "        # print(\"{:-<65} {}\".format(sentence, str(vs)))\n",
    "    return np.mean(compound1),np.mean(compound2)\n",
    "\n",
    "song_Vader('youll-be-mine')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a57bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in communities_d:\n",
    "    compound1=[]\n",
    "    compound2=[]\n",
    "    for el in communities_d[idx]:\n",
    "        print(compound1.append(song_Vader(el)[0]))\n",
    "        compound2.append(song_Vader(el)[1])\n",
    "    print(np.mean(compound1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368ab3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceDic = {}\n",
    "for name in dialogues:\n",
    "    sentenceDic[name] = []\n",
    "    for line in dialogues[name]:\n",
    "        sentence = re.split(r'[.!?]+ *',line)\n",
    "        sentenceDic[name].extend(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f801c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score_VADER(sentence):\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    return sentiment_dict['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4166b7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_score_VADER('Oh yes, you\\'ll be.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6f4699",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in communities_d:\n",
    "    for el in communities_d[idx]:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c99686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
